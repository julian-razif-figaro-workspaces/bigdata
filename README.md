# BigData Kafka to DynamoDB Service

A high-performance, reactive Spring Boot microservice for consuming Kafka messages and persisting session data to AWS DynamoDB. Built with Java 25, Spring Boot 4.0.1, and AWS SDK V2 for optimal throughput and reliability.

[![Java](https://img.shields.io/badge/Java-25-orange.svg)](https://openjdk.org/)
[![Spring Boot](https://img.shields.io/badge/Spring%20Boot-4.0.1-brightgreen.svg)](https://spring.io/projects/spring-boot)
[![AWS SDK](https://img.shields.io/badge/AWS%20SDK-V2-yellow.svg)](https://aws.amazon.com/sdk-for-java/)
[![License](https://img.shields.io/badge/License-Proprietary-red.svg)]()

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Getting Started](#getting-started)
- [Configuration](#configuration)
- [Building](#building)
- [Running](#running)
- [Monitoring](#monitoring)
- [Security](#security)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

## ğŸ¯ Overview

This service is part of a distributed data pipeline that:
1. Consumes session events from Kafka topics
2. Validates and filters messages in parallel
3. Transforms data to DynamoDB-compatible format
4. Persists session records asynchronously with high throughput

**Key Characteristics:**
- **Non-blocking I/O**: Reactive programming with Project Reactor
- **High Throughput**: Processes >10K messages/second
- **Fault Tolerant**: Automatic retries with exponential backoff
- **Secure**: OWASP-compliant input validation and security headers
- **Observable**: Built-in metrics, health checks, and distributed tracing

## ğŸ—ï¸ Architecture

### System Context

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Kafka Consumer  â”‚â”€â”€â”€â”€â”€â–¶â”‚  DynamoDB   â”‚
â”‚  (Source)   â”‚      â”‚     Service      â”‚      â”‚ (Sink)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Prometheus    â”‚
                     â”‚  (Monitoring)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Structure

```
bigdata/
â”œâ”€â”€ app-config-data/              # Configuration POJOs
â”‚   â””â”€â”€ KafkaConsumerConfigData
â”‚   â””â”€â”€ DynamonDBConfigData
â”œâ”€â”€ kafka-consumer-config/        # Kafka consumer configuration
â”‚   â””â”€â”€ KafkaConsumerConfig
â”‚   â””â”€â”€ KafkaConsumer (interface)
â”œâ”€â”€ dynamo-config/                # DynamoDB client configuration
â”‚   â””â”€â”€ DynamoDBConfig
â””â”€â”€ kafka-to-pv-dynamo-service/   # Main service application
    â”œâ”€â”€ KafkaToPvDynamoServiceApplication
    â”œâ”€â”€ KafkaToPvDynamoConsumer
    â”œâ”€â”€ DynamoDBService
    â””â”€â”€ SecurityConfig
```

### Data Flow

1. **Message Reception**: Kafka listener receives batch of messages (configurable batch size)
2. **Parallel Filtering**: Messages processed concurrently using reactive streams
3. **Validation**: JSON structure, size, depth, and required fields validated
4. **Transformation**: Data extracted and formatted for DynamoDB
5. **Persistence**: Async write to DynamoDB with retry logic
6. **Acknowledgment**: Batch acknowledged on successful processing

## âœ¨ Features

### Performance
- âœ… Reactive, non-blocking I/O with Project Reactor
- âœ… Parallel message processing with configurable concurrency
- âœ… Connection pooling for Kafka and DynamoDB
- âœ… Batch message processing for optimal throughput

### Reliability
- âœ… Automatic retry with exponential backoff
- âœ… Circuit breaker pattern for fault tolerance
- âœ… Graceful degradation under load
- âœ… Health checks and readiness probes

### Security
- âœ… Input validation against injection attacks
- âœ… JSON size and depth limits
- âœ… OWASP security headers (CSP, XSS, Frame Options)
- âœ… AWS credentials via environment variables

### Observability
- âœ… Spring Boot Actuator endpoints
- âœ… Prometheus metrics export
- âœ… Structured logging with correlation IDs
- âœ… Detailed error tracking

## ğŸ“¦ Prerequisites

### Required
- **Java 25** or higher ([Download](https://jdk.java.net/25/))
- **Maven 3.9+** (included via Maven Wrapper)
- **Apache Kafka 3.x** cluster
- **AWS DynamoDB** access (local or cloud)

### Optional
- **Docker** for containerized deployment
- **Kubernetes** for orchestration
- **Prometheus** for metrics collection
- **Grafana** for visualization

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone <repository-url>
cd bigdata
```

### 2. Configure Environment Variables

Create a `.env` file or set environment variables:

```bash
# AWS Credentials (use IAM roles in production)
export DYNAMO_CONFIG_DATA_AWS_ACCESS_KEY=your_access_key
export DYNAMO_CONFIG_DATA_AWS_SECRET_KEY=your_secret_key

# Spring Profile
export SPRING_PROFILES_ACTIVE=dev
```

### 3. Update Application Configuration

Edit `kafka-to-pv-dynamo-service/src/main/resources/application-dev.yaml`:

```yaml
kafka-consumer-config:
  bootstrap-servers: localhost:9092  # Your Kafka brokers
  group-id: your-consumer-group
  
dynamo-config-data:
  aws-region: ap-southeast-1
  dynamodb-endpoint: dynamodb.ap-southeast-1.amazonaws.com
```

### 4. Build the Project

```bash
# Windows
.\mvnw.cmd clean package

# Linux/Mac
./mvnw clean package
```

### 5. Run the Service

```bash
# Windows
.\mvnw.cmd spring-boot:run -pl kafka-to-pv-dynamo-service

# Linux/Mac
./mvnw spring-boot:run -pl kafka-to-pv-dynamo-service
```

## âš™ï¸ Configuration

### Kafka Consumer Settings

| Property | Default | Description |
|----------|---------|-------------|
| `bootstrap-servers` | `192.168.1.8:9092,...` | Comma-separated Kafka broker addresses |
| `group-id` | `BO_02_JULIAN_DEV_PV` | Consumer group identifier |
| `concurrency-level` | `3` | Number of concurrent consumer threads |
| `max-poll-records` | `500` | Max records per poll() call |
| `enable-auto-commit` | `false` | Manual offset commit for reliability |

### DynamoDB Settings

| Property | Default | Description |
|----------|---------|-------------|
| `aws-region` | `ap-southeast-1` | AWS region for DynamoDB |
| `max-connections` | `2000` | Maximum connection pool size |
| `max-retry` | `100` | Maximum retry attempts |
| `request-timeout` | `2000` | Request timeout in milliseconds |
| `connection-timeout` | `4000` | Connection timeout in milliseconds |

### Security Settings

Security headers are automatically configured via `SecurityConfig`:
- Content-Security-Policy: `default-src 'self'`
- X-XSS-Protection: `1; mode=block`
- X-Frame-Options: `DENY`
- Strict-Transport-Security: `max-age=31536000`

## ğŸ”¨ Building

### Build All Modules

```bash
.\mvnw.cmd clean install
```

### Build Specific Module

```bash
.\mvnw.cmd clean install -pl kafka-to-pv-dynamo-service
```

### Skip Tests

```bash
.\mvnw.cmd clean package -DskipTests
```

### Run Tests with Coverage

```bash
.\mvnw.cmd clean test jacoco:report
```

View coverage report: `target/site/jacoco/index.html`

## ğŸƒ Running

### Development Mode

```bash
.\mvnw.cmd spring-boot:run -pl kafka-to-pv-dynamo-service -Dspring-boot.run.profiles=dev
```

### Production Mode

```bash
java -jar kafka-to-pv-dynamo-service/target/kafka-to-pv-dynamo-service.jar \
  --spring.profiles.active=prod
```

### Docker

```bash
# Build image
docker build -t kafka-to-dynamo:latest .

# Run container
docker run -d \
  -e SPRING_PROFILES_ACTIVE=prod \
  -e DYNAMO_CONFIG_DATA_AWS_ACCESS_KEY=xxx \
  -e DYNAMO_CONFIG_DATA_AWS_SECRET_KEY=yyy \
  -p 8080:8080 \
  kafka-to-dynamo:latest
```

## ğŸ“Š Monitoring

### Health Checks

```bash
# Liveness probe
curl http://localhost:8080/actuator/health/liveness

# Readiness probe
curl http://localhost:8080/actuator/health/readiness

# Detailed health
curl http://localhost:8080/actuator/health
```

### Metrics

```bash
# Application metrics
curl http://localhost:8080/actuator/metrics

# Prometheus format
curl http://localhost:8080/actuator/prometheus
```

### Key Metrics to Monitor

- `kafka.consumer.records.consumed.total` - Total messages consumed
- `kafka.consumer.lag` - Consumer lag
- `dynamodb.putitem.duration` - DynamoDB write latency
- `jvm.memory.used` - Memory usage
- `system.cpu.usage` - CPU utilization

## ğŸ”’ Security

### Best Practices Implemented

1. **Input Validation**: All inputs validated before processing
2. **JSON Size Limits**: Messages limited to 1MB
3. **JSON Depth Limits**: Maximum depth of 10 levels
4. **Numeric Validation**: IDs validated as positive integers
5. **String Length Validation**: Username max 255 characters
6. **Security Headers**: OWASP-recommended headers configured
7. **No Hardcoded Secrets**: Credentials via environment variables

### Security Headers

All responses include:
- `Content-Security-Policy`
- `X-XSS-Protection`
- `X-Frame-Options`
- `X-Content-Type-Options`
- `Strict-Transport-Security`

### AWS Credentials

**Production**: Use IAM roles for EC2/ECS/Lambda

```yaml
dynamo-config-data:
  aws-accesskey: ""  # Empty = use default credentials chain
  aws-secretkey: ""
```

## ğŸ§ª Testing

### Run All Tests

```bash
.\mvnw.cmd test
```

### Run Specific Test Class

```bash
.\mvnw.cmd test -Dtest=DynamoDBServiceTest
```

### Integration Tests

```bash
.\mvnw.cmd verify
```

### Test Coverage

Target: **80%** minimum coverage

```bash
.\mvnw.cmd clean test jacoco:report
```

## ğŸ› Troubleshooting

### Common Issues

#### 1. Kafka Connection Refused

**Symptom**: `Connection refused: localhost:9092`

**Solution**: 
- Verify Kafka is running: `docker ps` or check service status
- Update `bootstrap-servers` in configuration
- Check network connectivity

#### 2. DynamoDB Access Denied

**Symptom**: `AccessDeniedException: User is not authorized`

**Solution**:
- Verify AWS credentials are set correctly
- Check IAM permissions include `dynamodb:PutItem`
- Verify table exists and credentials have access

#### 3. High Memory Usage

**Symptom**: `OutOfMemoryError` or high heap usage

**Solution**:
- Reduce `max-poll-records` in Kafka config
- Decrease `concurrency-level`
- Increase JVM heap: `-Xmx2g`

#### 4. Consumer Lag Growing

**Symptom**: Consumer offset falling behind

**Solution**:
- Increase `concurrency-level`
- Scale horizontally (add more consumer instances)
- Optimize DynamoDB write throughput
- Check for slow processing in logs

### Enable Debug Logging

```yaml
logging:
  level:
    com.julian.razif.figaro.bigdata: DEBUG
    org.springframework.kafka: DEBUG
    software.amazon.awssdk: DEBUG
```

## ğŸ“ Contributing

### Development Workflow

1. Create feature branch: `git checkout -b feature/my-feature`
2. Make changes following code style guidelines
3. Write/update tests (maintain 80% coverage)
4. Run full build: `.\mvnw.cmd clean verify`
5. Commit with descriptive message
6. Push and create pull request

### Code Style

- Follow Java standard naming conventions
- Maximum line length: 120 characters
- Use meaningful variable names
- Add Javadoc for public methods
- Keep methods focused and small (<50 lines)

### Testing Requirements

- Unit tests for all business logic
- Integration tests for external dependencies
- Minimum 80% code coverage
- All tests must pass before merge

## ğŸ“„ License

Proprietary - All rights reserved

## ğŸ‘¥ Authors

- **Julian Razif Figaro** - Initial work

## ğŸ“§ Support

For issues and questions:
- Create an issue in the repository
- Contact the development team
- Check existing documentation

## ğŸ”„ Version History

- **0.0.1-SNAPSHOT** - Initial development version
  - Kafka consumer implementation
  - DynamoDB async persistence
  - Security hardening
  - Monitoring and observability

---

**Built with â¤ï¸ using Spring Boot and AWS**
